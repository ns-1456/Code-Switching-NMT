{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English → Hinglish Neural Machine Translation\n",
        "### From-Scratch Transformer on Google Colab Pro\n",
        "\n",
        "This notebook runs the full pipeline:\n",
        "1. **Setup** — Install deps, mount Drive, clone repo\n",
        "2. **Phase 1** — Data pipeline (download, clean, split)\n",
        "3. **Phase 2** — Train custom BPE tokenizer\n",
        "4. **Phase 3** — Build model and verify parameter count\n",
        "5. **Phase 4** — Train the Transformer\n",
        "6. **Phase 5** — Evaluate (BLEU, chrF, vibe check)\n",
        "7. **Save** — Checkpoint to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (for saving checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "# !git clone https://github.com/ns-1456/Code-Switching-NMT.git\n",
        "# %cd Code-Switching-NMT\n",
        "\n",
        "# OR: If you uploaded the code to Drive:\n",
        "# !cp -r \"/content/drive/MyDrive/Code-Switching NMT\" /content/project\n",
        "# %cd /content/project\n",
        "\n",
        "# For now, assume we're in the project root\n",
        "import os\n",
        "# Uncomment and set the correct path:\n",
        "# os.chdir('/content/Code-Switching-NMT')\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch tokenizers transformers datasets pandas sacrebleu pyyaml tqdm matplotlib seaborn streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify imports\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Data Pipeline\n",
        "\n",
        "Downloads `findnitai/english-to-hinglish` (189k rows), cleans it, and splits 90/5/5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data_pipeline import load_config\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Define clean_dataframe and split_dataframe locally\n",
        "def clean_dataframe(df, min_words, max_words):\n",
