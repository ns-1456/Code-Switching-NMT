{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English → Hinglish Neural Machine Translation\n",
        "### From-Scratch Transformer on Google Colab Pro\n",
        "\n",
        "This notebook runs the full pipeline:\n",
        "1. **Setup** — Install deps, mount Drive, clone repo\n",
        "2. **Phase 1** — Data pipeline (download, clean, split)\n",
        "3. **Phase 2** — Train custom BPE tokenizer\n",
        "4. **Phase 3** — Build model and verify parameter count\n",
        "5. **Phase 4** — Train the Transformer\n",
        "6. **Phase 5** — Evaluate (BLEU, chrF, vibe check)\n",
        "7. **Save** — Checkpoint to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (for saving checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "# !git clone https://github.com/ns-1456/Code-Switching-NMT.git\n",
        "# %cd Code-Switching-NMT\n",
        "\n",
        "# OR: If you uploaded the code to Drive:\n",
        "# !cp -r \"/content/drive/MyDrive/Code-Switching NMT\" /content/project\n",
        "# %cd /content/project\n",
        "\n",
        "# For now, assume we're in the project root\n",
        "import os\n",
        "# Uncomment and set the correct path:\n",
        "# os.chdir('/content/Code-Switching-NMT')\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch tokenizers transformers datasets pandas sacrebleu pyyaml tqdm matplotlib seaborn streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify imports\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Data Pipeline\n",
        "\n",
        "Downloads `findnitai/english-to-hinglish` (189k rows), cleans it, and splits 90/5/5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data_pipeline import load_config\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Define clean_dataframe and split_dataframe locally\n",
        "def clean_dataframe(df, min_words, max_words):\n",
        "    \"\"\"Cleans the dataframe based on sentence length.\"\"\"\n",
        "    initial_rows = len(df)\n",
        "    df['en_len'] = df['en'].apply(lambda x: len(x.split()))\n",
        "    df['hi_len'] = df['hi_ng'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    df = df[\n",
        "        (df['en_len'] >= min_words) & (df['en_len'] <= max_words) &\n",
        "        (df['hi_len'] >= min_words) & (df['hi_len'] <= max_words)\n",
        "    ]\n",
        "    df = df.drop(columns=['en_len', 'hi_len'])\n",
        "    print(f\"Filtered {initial_rows - len(df)} rows. Remaining: {len(df):,}\")\n",
        "    return df\n",
        "\n",
        "def split_dataframe(df, train_ratio, val_ratio, seed):\n",
        "    \"\"\"Splits the dataframe into training, validation, and test sets.\"\"\"\n",
        "    # Calculate test ratio\n",
        "    test_ratio = 1.0 - train_ratio - val_ratio\n",
        "\n",
        "    # Split train and temp (val + test)\n",
        "    train_df, temp_df = train_test_split(df, test_size=(val_ratio + test_ratio), random_state=seed)\n",
        "\n",
        "    # Split val and test from temp\n",
        "    # Adjust val_ratio for the temp_df split\n",
        "    if (val_ratio + test_ratio) > 0:\n",
        "        val_test_ratio = val_ratio / (val_ratio + test_ratio)\n",
        "        val_df, test_df = train_test_split(temp_df, test_size=test_ratio / (val_ratio + test_ratio), random_state=seed)\n",
        "    else:\n",
        "        val_df = pd.DataFrame(columns=df.columns)\n",
        "        test_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
