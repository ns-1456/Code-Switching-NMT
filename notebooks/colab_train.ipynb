{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English → Hinglish Neural Machine Translation\n",
        "### From-Scratch Transformer on Google Colab Pro\n",
        "\n",
        "This notebook runs the full pipeline:\n",
        "1. **Setup** — Install deps, mount Drive, clone repo\n",
        "2. **Phase 1** — Data pipeline (download, clean, split)\n",
        "3. **Phase 2** — Train custom BPE tokenizer\n",
        "4. **Phase 3** — Build model and verify parameter count\n",
        "5. **Phase 4** — Train the Transformer\n",
        "6. **Phase 5** — Evaluate (BLEU, chrF, vibe check)\n",
        "7. **Save** — Checkpoint to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (for saving checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "# !git clone https://github.com/ns-1456/Code-Switching-NMT.git\n",
        "# %cd Code-Switching-NMT\n",
        "\n",
        "# OR: If you uploaded the code to Drive:\n",
        "# !cp -r \"/content/drive/MyDrive/Code-Switching NMT\" /content/project\n",
        "# %cd /content/project\n",
        "\n",
        "# For now, assume we're in the project root\n",
        "import os\n",
        "# Uncomment and set the correct path:\n",
        "# os.chdir('/content/Code-Switching-NMT')\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch tokenizers transformers datasets pandas sacrebleu pyyaml tqdm matplotlib seaborn streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify imports\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Data Pipeline\n",
        "\n",
        "Downloads `findnitai/english-to-hinglish` (189k rows), cleans it, and splits 90/5/5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data_pipeline import load_config\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Define clean_dataframe and split_dataframe locally\n",
        "def clean_dataframe(df, min_words, max_words):\n",
        "    \"\"\"Cleans the dataframe based on sentence length.\"\"\"\n",
        "    initial_rows = len(df)\n",
        "    df['en_len'] = df['en'].apply(lambda x: len(x.split()))\n",
        "    df['hi_len'] = df['hi_ng'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    df = df[\n",
        "        (df['en_len'] >= min_words) & (df['en_len'] <= max_words) &\n",
        "        (df['hi_len'] >= min_words) & (df['hi_len'] <= max_words)\n",
        "    ]\n",
        "    df = df.drop(columns=['en_len', 'hi_len'])\n",
        "    print(f\"Filtered {initial_rows - len(df)} rows. Remaining: {len(df):,}\")\n",
        "    return df\n",
        "\n",
        "def split_dataframe(df, train_ratio, val_ratio, seed):\n",
        "    \"\"\"Splits the dataframe into training, validation, and test sets.\"\"\"\n",
        "    # Calculate test ratio\n",
        "    test_ratio = 1.0 - train_ratio - val_ratio\n",
        "\n",
        "    # Split train and temp (val + test)\n",
        "    train_df, temp_df = train_test_split(df, test_size=(val_ratio + test_ratio), random_state=seed)\n",
        "\n",
        "    # Split val and test from temp\n",
        "    # Adjust val_ratio for the temp_df split\n",
        "    if (val_ratio + test_ratio) > 0:\n",
        "        val_test_ratio = val_ratio / (val_ratio + test_ratio)\n",
        "        val_df, test_df = train_test_split(temp_df, test_size=test_ratio / (val_ratio + test_ratio), random_state=seed)\n",
        "    else:\n",
        "        val_df = pd.DataFrame(columns=df.columns)\n",
        "        test_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "config = load_config('configs/config.yaml')\n",
        "\n",
        "# Update config with the actual column names after extraction\n",
        "config['data']['en_col'] = 'en'\n",
        "config['data']['hi_col'] = 'hi_ng'\n",
        "\n",
        "# Extract necessary parameters from config\n",
        "dataset_name = config[\"data\"][\"dataset_name\"]\n",
        "min_words = config[\"data\"][\"min_words\"]\n",
        "max_words = config[\"data\"][\"max_words\"]\n",
        "train_ratio = config[\"data\"][\"train_ratio\"]\n",
        "val_ratio = config[\"data\"][\"val_ratio\"]\n",
        "seed = config[\"data\"][\"seed\"]\n",
        "data_dir = config[\"data\"][\"data_dir\"]\n",
        "\n",
        "print(f\"[data] Downloading {dataset_name} ...\")\n",
        "raw_dataset = load_dataset(dataset_name)\n",
        "df = raw_dataset[\"train\"].to_pandas()\n",
        "print(f\"[data] Raw rows: {len(df):,}\")\n",
        "\n",
        "# Extract 'en' and 'hi_ng' from the 'translation' column\n",
        "if 'translation' in df.columns:\n",
        "    df['en'] = df['translation'].apply(lambda x: x['en'])\n",
        "    df['hi_ng'] = df['translation'].apply(lambda x: x['hi_ng'])\n",
        "    # Select only the extracted columns and ensure standard names\n",
        "    df = df[['en', 'hi_ng']].copy()\n",
        "else:\n",
        "    raise ValueError(\"The dataset does not contain a 'translation' column as expected.\")\n",
        "\n",
        "# Apply cleaning\n",
        "df = clean_dataframe(df, min_words, max_words)\n",
        "print(f\"[data] Cleaned rows: {len(df):,}\")\n",
        "\n",
        "# Split the dataframe\n",
        "train_df, val_df, test_df = split_dataframe(\n",
        "    df, train_ratio, val_ratio, seed\n",
        ")\n",
        "\n",
        "# Ensure data directory exists and save DataFrames\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "train_df.to_csv(os.path.join(data_dir, 'train.csv'), index=False)\n",
        "val_df.to_csv(os.path.join(data_dir, 'val.csv'), index=False)\n",
        "test_df.to_csv(os.path.join(data_dir, 'test.csv'), index=False)\n",
        "print(f\"[data] Saved train, val, test datasets to {data_dir}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick data inspection\n",
        "print(f\"Train: {len(train_df):,} rows\")\n",
        "print(f\"Val:   {len(val_df):,} rows\")\n",
        "print(f\"Test:  {len(test_df):,} rows\")\n",
        "print()\n",
        "print(\"Sample rows:\")\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 2: Train Custom BPE Tokenizer\n",
        "\n",
        "Trains a ByteLevelBPE tokenizer (vocab=16k) on the combined English + Hinglish training corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.tokenizer import train_tokenizer, encode, decode\n",
        "\n",
        "tokenizer = train_tokenizer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke test the tokenizer\n",
        "test_sents = [\n",
        "    \"I am going home\",\n",
        "    \"kal milte hai\",\n",
        "    \"let's go to the market bhaiya\",\n",
        "    \"tu pagal hai kya\",\n",
        "]\n",
        "\n",
        "for sent in test_sents:\n",
        "    ids = encode(sent, tokenizer)\n",
        "    decoded = decode(ids, tokenizer)\n",
        "    print(f\"  '{sent}'\")\n",
        "    print(f\"    -> IDs: {ids[:12]}{'...' if len(ids) > 12 else ''}\")\n",
        "    print(f\"    -> Decoded: '{decoded}'\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 3: Build Model\n",
        "\n",
        "From-scratch Encoder-Decoder Transformer (4L/4L, d_model=256, 8 heads)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model import build_model\n",
        "\n",
        "model = build_model(config, vocab_size=tokenizer.vocab_size)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters:     {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size (fp32):    {total_params * 4 / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 4: Train\n",
        "\n",
        "Training loop with teacher forcing, AdamW + warmup/cosine scheduler, label smoothing, and early stopping.\n",
        "\n",
        "**Expected time: ~2-3 hours on T4, ~1-1.5 hours on A100.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.train import train\n",
        "\n",
        "# Run training (uses config.yaml settings)\n",
        "trained_model = train(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 5: Evaluate\n",
        "\n",
        "Compute BLEU + chrF on the test set. Run qualitative vibe check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.evaluate import run_evaluation\n",
        "\n",
        "# Corpus eval uses fast batched greedy (~2 min vs ~3 hours)\n",
        "# Vibe check uses beam search (only 20 sentences, quality matters)\n",
        "eval_results, vibe_results = run_evaluation(\n",
        "    checkpoint_path='checkpoints/best_model.pt',\n",
        "    config_path='configs/config.yaml',\n",
        "    corpus_method='greedy',   # fast batched GPU decoding\n",
        "    vibe_method='beam',       # beam search for the 20-sentence quality check\n",
        "    beam_width=5,\n",
        "    batch_size=128,\n",
        ")"
      ]
